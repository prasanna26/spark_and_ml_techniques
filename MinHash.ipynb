{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/Cellar/apache-spark/2.4.4/libexec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bharaths-mbp:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test_newwork</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1110e5c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, IDF, MinHashLSH\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"test_newwork\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bharaths-mbp:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test_newwork</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=test_newwork>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the textfiles \n",
    "data=sc.wholeTextFiles(\"/Users/bharathsurianarayanan/Downloads/cookbook_text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      "  Henrie...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      "   Cooki...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      "   \r\n",
      "   ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " The ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " The ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      "\r\n",
      " \r\n",
      "  The ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      "  \r\n",
      " \r\n",
      " The...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " Brea...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      "  \r\n",
      " \r\n",
      " ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " The ...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      "  The...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " \r",
      "...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      "  The...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " Manu...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " Comm...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " \r",
      "...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " Favo...|\n",
      "|file:/Users/bhara...|The White House C...|\n",
      "|file:/Users/bhara...|The Young House-K...|\n",
      "|file:/Users/bhara...| \r\n",
      " \r\n",
      " \r\n",
      " \r\n",
      " The ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe using the loaded data\n",
    "df = spark.createDataFrame(data)\n",
    "# display the dataframe df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           File_name|\n",
      "+--------------------+\n",
      "|file:/Users/bhara...|\n",
      "|file:/Users/bhara...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# rename the column 1 to FIle_name\n",
    "newdf = df.withColumnRenamed(\"_1\", \"File_name\")\n",
    "# rename the column 2 to text\n",
    "newdf = newdf.withColumnRenamed(\"_2\", \"text\")\n",
    "newdf.select('File_name').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- File_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|           File_name|                text|\n",
      "+--------------------+--------------------+\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      "  Henrie...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      "   Cooki...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      "   \n",
      "   ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " The ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " The ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      "\n",
      " \n",
      "  The ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      "  \n",
      " \n",
      " The...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " Brea...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " The ...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      "  The...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      "  The...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " Manu...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " Comm...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " Favo...|\n",
      "|file:/Users/bhara...|The White House C...|\n",
      "|file:/Users/bhara...|The Young House-K...|\n",
      "|file:/Users/bhara...| \n",
      " \n",
      " \n",
      " \n",
      " The ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display yhe schema and the top 20 rows of the newdf\n",
    "newdf.printSchema()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- File_name: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+---------+--------------------+\n",
      "|File_name|            features|\n",
      "+---------+--------------------+\n",
      "| henr.txt|(65008,[0,1,2,3,4...|\n",
      "| cclu.txt|(65008,[0,1,2,3,4...|\n",
      "| beec.txt|(65008,[0,1,2,3,4...|\n",
      "| hous.txt|(65008,[0,1,2,3,4...|\n",
      "| virg.txt|(65008,[0,1,2,3,4...|\n",
      "| ldnw.txt|(65008,[0,1,2,3,4...|\n",
      "| conf.txt|(65008,[0,1,2,3,4...|\n",
      "| brkf.txt|(65008,[0,1,2,3,4...|\n",
      "| buck.txt|(65008,[0,1,2,3,4...|\n",
      "| coow.txt|(65008,[0,1,2,3,4...|\n",
      "| grea.txt|(65008,[0,1,2,3,4...|\n",
      "| blue.txt|(65008,[0,1,2,3,4...|\n",
      "| frch.txt|(65008,[0,1,2,3,4...|\n",
      "| army.txt|(65008,[0,1,2,3,4...|\n",
      "| comm.txt|(65008,[0,1,2,3,4...|\n",
      "| bost.txt|(65008,[0,1,2,3,4...|\n",
      "| favd.txt|(65008,[0,1,2,3,4...|\n",
      "| whit.txt|(65008,[0,1,2,3,4...|\n",
      "| youn.txt|(65008,[0,1,2,3,4...|\n",
      "| neig.txt|(65008,[0,1,2,3,4...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform processing so as to remove the directory name from the file name\n",
    "newdf = newdf.withColumn('File_name', regexp_replace('File_name', 'file:/Users/bharathsurianarayanan/Downloads/cookbook_text/', ''))\n",
    "# tokenize the column text and name it words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "newdf.printSchema()\n",
    "# newdf.show()\n",
    "# \n",
    "wordsDf = tokenizer.transform(newdf)\n",
    "\n",
    "vocabSize = 1000000\n",
    "# convert the text document to vectors using the countVectorizer so as to vectorize the words column\n",
    "# the vectorized column is now named features\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=vocabSize, minDF=2).fit(wordsDf)\n",
    "vectorizedDf = cv.transform(wordsDf).select(col(\"File_name\"), col(\"features\"))\n",
    "vectorizedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hashes for the vectorized features column, this hash is thus used for comparing text files\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "# build the model for the vectorizeddf\n",
    "model = mh.fit(vectorizedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---------+--------------------+--------------------+\n",
      "|File_name|            features|              hashes|\n",
      "+---------+--------------------+--------------------+\n",
      "| henr.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| cclu.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| beec.txt|(65008,[0,1,2,3,4...|[[69017.0], [1946...|\n",
      "| hous.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| virg.txt|(65008,[0,1,2,3,4...|[[178754.0], [192...|\n",
      "| ldnw.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| conf.txt|(65008,[0,1,2,3,4...|[[178754.0], [692...|\n",
      "| brkf.txt|(65008,[0,1,2,3,4...|[[178754.0], [192...|\n",
      "| buck.txt|(65008,[0,1,2,3,4...|[[69017.0], [1946...|\n",
      "| coow.txt|(65008,[0,1,2,3,4...|[[69017.0], [1946...|\n",
      "| grea.txt|(65008,[0,1,2,3,4...|[[178754.0], [692...|\n",
      "| blue.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| frch.txt|(65008,[0,1,2,3,4...|[[178754.0], [192...|\n",
      "| army.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| comm.txt|(65008,[0,1,2,3,4...|[[128297.0], [194...|\n",
      "| bost.txt|(65008,[0,1,2,3,4...|[[308251.0], [194...|\n",
      "| favd.txt|(65008,[0,1,2,3,4...|[[178754.0], [295...|\n",
      "| whit.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "| youn.txt|(65008,[0,1,2,3,4...|[[69017.0], [1946...|\n",
      "| neig.txt|(65008,[0,1,2,3,4...|[[178754.0], [194...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(vectorizedDf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|           distCol|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[bost.txt, (65008...|[lady.txt, (65008...|0.7293906810035842|\n",
      "|[cclu.txt, (65008...|[hosf.txt, (65008...|0.7473989595838335|\n",
      "|[prho.txt, (65008...|[amwh.txt, (65008...|0.6818581477139507|\n",
      "|[time.txt, (65008...|[buck.txt, (65008...|0.7516558232225543|\n",
      "|[jewi.txt, (65008...|[buck.txt, (65008...|0.7083195043699524|\n",
      "|[engl.txt, (65008...|[miss.txt, (65008...|0.7498535442296426|\n",
      "|[lady.txt, (65008...|[notm.txt, (65008...| 0.726773575039219|\n",
      "|[econ.txt, (65008...|[pcdg.txt, (65008...|0.7564141722663409|\n",
      "|[favd.txt, (65008...|[fran.txt, (65008...|0.7800912721049629|\n",
      "|[good.txt, (65008...|[jenn.txt, (65008...|0.7117792195924335|\n",
      "|[matf.txt, (65008...|[mary.txt, (65008...|0.7983123004409305|\n",
      "|[miss.txt, (65008...|[wash.txt, (65008...| 0.682782801254844|\n",
      "|[neig.txt, (65008...|[pres.txt, (65008...|0.7004736051750029|\n",
      "|[sett.txt, (65008...|[swed.txt, (65008...|0.6930379746835442|\n",
      "|[zuni.txt, (65008...|[beec.txt, (65008...|0.7700305810397554|\n",
      "|[chas.txt, (65008...|[ency.txt, (65008...|0.7185407790626902|\n",
      "|[scie.txt, (65008...|[buck.txt, (65008...|0.6546970182750882|\n",
      "|[chin.txt, (65008...|[pres.txt, (65008...|0.7739006587146163|\n",
      "|[lady.txt, (65008...|[pres.txt, (65008...|0.7451014849310253|\n",
      "|[lady.txt, (65008...|[swed.txt, (65008...|0.7419801335287413|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the threshold and then used the built model to calculate the similarity index for all the files\n",
    "# all files having similarity greater than the threshold will be displayed\n",
    "threshold = 0.8\n",
    "comparison=model.approxSimilarityJoin(vectorizedDf, vectorizedDf, threshold).filter(\"distCol != 0\")\n",
    "comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3249"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of columns having similarity greater than 0.8\n",
    "comparison.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
