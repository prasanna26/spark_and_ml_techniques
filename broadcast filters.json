{"paragraphs":[{"text":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nval spark: SparkSession = SparkSession.builder()\n    .master(\"local[*]\")\n    .appName(\"simple-app\")\n    .getOrCreate()","user":"anonymous","dateUpdated":"2019-12-03T23:06:23-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1b5603ed\n"}]},"apps":[],"jobName":"paragraph_1574791774863_-577846731","id":"20191126-130934_315782510","dateCreated":"2019-11-26T13:09:34-0500","dateStarted":"2019-12-03T23:06:23-0500","dateFinished":"2019-12-03T23:06:24-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5245"},{"text":"// code attribution: refernces : https://stackoverflow.com/questions/41988709/using-stat-bloomfilter-in-spark-2-0-0-to-filter-another-dataframe","user":"anonymous","dateUpdated":"2019-12-03T23:06:24-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575411421217_412171792","id":"20191203-171701_119194562","dateCreated":"2019-12-03T17:17:01-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5246","dateFinished":"2019-12-03T23:06:24-0500","dateStarted":"2019-12-03T23:06:24-0500","results":{"code":"SUCCESS","msg":[]}},{"text":"val sc=spark.sparkContext\nsc","user":"anonymous","dateUpdated":"2019-12-03T23:06:24-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@72eae44d\nres4: org.apache.spark.SparkContext = org.apache.spark.SparkContext@72eae44d\n"}]},"apps":[],"jobName":"paragraph_1574794511214_-1780558832","id":"20191126-135511_35476987","dateCreated":"2019-11-26T13:55:11-0500","dateStarted":"2019-12-03T23:06:24-0500","dateFinished":"2019-12-03T23:06:24-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5247"},{"text":"// load the data of all the csv files except the file 2019-03-31.csv, the folder : drive_stats_2019_Q1_temp contains all files except the 2019-03-31.csv file\n// this dataframe is dataframe df_A, the larger data\nval df_A = spark.read.option(\"header\", \"true\").csv(\"/Users/bharathsurianarayanan/Downloads/drive_stats_2019_Q1_temp/*.csv\")","user":"anonymous","dateUpdated":"2019-12-03T23:06:24-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_A: org.apache.spark.sql.DataFrame = [date: string, serial_number: string ... 127 more fields]\n"}]},"apps":[],"jobName":"paragraph_1574795102331_-1321285504","id":"20191126-140502_601201244","dateCreated":"2019-11-26T14:05:02-0500","dateStarted":"2019-12-03T23:06:24-0500","dateFinished":"2019-12-03T23:06:28-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5248"},{"text":"df_A.count()","user":"anonymous","dateUpdated":"2019-12-03T23:06:28-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res5: Long = 9470808\n"}]},"apps":[],"jobName":"paragraph_1574795190347_1109957545","id":"20191126-140630_845733281","dateCreated":"2019-11-26T14:06:30-0500","dateStarted":"2019-12-03T23:06:28-0500","dateFinished":"2019-12-03T23:06:32-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5249"},{"text":"// load the file 2019-03-31.csv into the dataframe df_B, this is the smaller data which needs to be broadcasted\nval df_B=spark.read.option(\"header\",\"true\").csv(\"/Users/bharathsurianarayanan/Downloads/drive_stats_2019_Q1/2019-03-31.csv\")","user":"anonymous","dateUpdated":"2019-12-03T23:06:32-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_B: org.apache.spark.sql.DataFrame = [date: string, serial_number: string ... 127 more fields]\n"}]},"apps":[],"jobName":"paragraph_1574795485625_776052842","id":"20191126-141125_1486987883","dateCreated":"2019-11-26T14:11:25-0500","dateStarted":"2019-12-03T23:06:32-0500","dateFinished":"2019-12-03T23:06:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5250"},{"text":"// initialize values expectedElements and fpp which are arguments for the bloomFilter method\nval expectedElements=df_B.count()\nval fpp:Double=0.005\n// lets have a look at the schema of this smaller dataframe\ndf_B.printSchema()","user":"anonymous","dateUpdated":"2019-12-03T23:06:33-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- date: string (nullable = true)\n |-- serial_number: string (nullable = true)\n |-- model: string (nullable = true)\n |-- capacity_bytes: string (nullable = true)\n |-- failure: string (nullable = true)\n |-- smart_1_normalized: string (nullable = true)\n |-- smart_1_raw: string (nullable = true)\n |-- smart_2_normalized: string (nullable = true)\n |-- smart_2_raw: string (nullable = true)\n |-- smart_3_normalized: string (nullable = true)\n |-- smart_3_raw: string (nullable = true)\n |-- smart_4_normalized: string (nullable = true)\n |-- smart_4_raw: string (nullable = true)\n |-- smart_5_normalized: string (nullable = true)\n |-- smart_5_raw: string (nullable = true)\n |-- smart_7_normalized: string (nullable = true)\n |-- smart_7_raw: string (nullable = true)\n |-- smart_8_normalized: string (nullable = true)\n |-- smart_8_raw: string (nullable = true)\n |-- smart_9_normalized: string (nullable = true)\n |-- smart_9_raw: string (nullable = true)\n |-- smart_10_normalized: string (nullable = true)\n |-- smart_10_raw: string (nullable = true)\n |-- smart_11_normalized: string (nullable = true)\n |-- smart_11_raw: string (nullable = true)\n |-- smart_12_normalized: string (nullable = true)\n |-- smart_12_raw: string (nullable = true)\n |-- smart_13_normalized: string (nullable = true)\n |-- smart_13_raw: string (nullable = true)\n |-- smart_15_normalized: string (nullable = true)\n |-- smart_15_raw: string (nullable = true)\n |-- smart_16_normalized: string (nullable = true)\n |-- smart_16_raw: string (nullable = true)\n |-- smart_17_normalized: string (nullable = true)\n |-- smart_17_raw: string (nullable = true)\n |-- smart_22_normalized: string (nullable = true)\n |-- smart_22_raw: string (nullable = true)\n |-- smart_23_normalized: string (nullable = true)\n |-- smart_23_raw: string (nullable = true)\n |-- smart_24_normalized: string (nullable = true)\n |-- smart_24_raw: string (nullable = true)\n |-- smart_168_normalized: string (nullable = true)\n |-- smart_168_raw: string (nullable = true)\n |-- smart_170_normalized: string (nullable = true)\n |-- smart_170_raw: string (nullable = true)\n |-- smart_173_normalized: string (nullable = true)\n |-- smart_173_raw: string (nullable = true)\n |-- smart_174_normalized: string (nullable = true)\n |-- smart_174_raw: string (nullable = true)\n |-- smart_177_normalized: string (nullable = true)\n |-- smart_177_raw: string (nullable = true)\n |-- smart_179_normalized: string (nullable = true)\n |-- smart_179_raw: string (nullable = true)\n |-- smart_181_normalized: string (nullable = true)\n |-- smart_181_raw: string (nullable = true)\n |-- smart_182_normalized: string (nullable = true)\n |-- smart_182_raw: string (nullable = true)\n |-- smart_183_normalized: string (nullable = true)\n |-- smart_183_raw: string (nullable = true)\n |-- smart_184_normalized: string (nullable = true)\n |-- smart_184_raw: string (nullable = true)\n |-- smart_187_normalized: string (nullable = true)\n |-- smart_187_raw: string (nullable = true)\n |-- smart_188_normalized: string (nullable = true)\n |-- smart_188_raw: string (nullable = true)\n |-- smart_189_normalized: string (nullable = true)\n |-- smart_189_raw: string (nullable = true)\n |-- smart_190_normalized: string (nullable = true)\n |-- smart_190_raw: string (nullable = true)\n |-- smart_191_normalized: string (nullable = true)\n |-- smart_191_raw: string (nullable = true)\n |-- smart_192_normalized: string (nullable = true)\n |-- smart_192_raw: string (nullable = true)\n |-- smart_193_normalized: string (nullable = true)\n |-- smart_193_raw: string (nullable = true)\n |-- smart_194_normalized: string (nullable = true)\n |-- smart_194_raw: string (nullable = true)\n |-- smart_195_normalized: string (nullable = true)\n |-- smart_195_raw: string (nullable = true)\n |-- smart_196_normalized: string (nullable = true)\n |-- smart_196_raw: string (nullable = true)\n |-- smart_197_normalized: string (nullable = true)\n |-- smart_197_raw: string (nullable = true)\n |-- smart_198_normalized: string (nullable = true)\n |-- smart_198_raw: string (nullable = true)\n |-- smart_199_normalized: string (nullable = true)\n |-- smart_199_raw: string (nullable = true)\n |-- smart_200_normalized: string (nullable = true)\n |-- smart_200_raw: string (nullable = true)\n |-- smart_201_normalized: string (nullable = true)\n |-- smart_201_raw: string (nullable = true)\n |-- smart_218_normalized: string (nullable = true)\n |-- smart_218_raw: string (nullable = true)\n |-- smart_220_normalized: string (nullable = true)\n |-- smart_220_raw: string (nullable = true)\n |-- smart_222_normalized: string (nullable = true)\n |-- smart_222_raw: string (nullable = true)\n |-- smart_223_normalized: string (nullable = true)\n |-- smart_223_raw: string (nullable = true)\n |-- smart_224_normalized: string (nullable = true)\n |-- smart_224_raw: string (nullable = true)\n |-- smart_225_normalized: string (nullable = true)\n |-- smart_225_raw: string (nullable = true)\n |-- smart_226_normalized: string (nullable = true)\n |-- smart_226_raw: string (nullable = true)\n |-- smart_231_normalized: string (nullable = true)\n |-- smart_231_raw: string (nullable = true)\n |-- smart_232_normalized: string (nullable = true)\n |-- smart_232_raw: string (nullable = true)\n |-- smart_233_normalized: string (nullable = true)\n |-- smart_233_raw: string (nullable = true)\n |-- smart_235_normalized: string (nullable = true)\n |-- smart_235_raw: string (nullable = true)\n |-- smart_240_normalized: string (nullable = true)\n |-- smart_240_raw: string (nullable = true)\n |-- smart_241_normalized: string (nullable = true)\n |-- smart_241_raw: string (nullable = true)\n |-- smart_242_normalized: string (nullable = true)\n |-- smart_242_raw: string (nullable = true)\n |-- smart_250_normalized: string (nullable = true)\n |-- smart_250_raw: string (nullable = true)\n |-- smart_251_normalized: string (nullable = true)\n |-- smart_251_raw: string (nullable = true)\n |-- smart_252_normalized: string (nullable = true)\n |-- smart_252_raw: string (nullable = true)\n |-- smart_254_normalized: string (nullable = true)\n |-- smart_254_raw: string (nullable = true)\n |-- smart_255_normalized: string (nullable = true)\n |-- smart_255_raw: string (nullable = true)\n\nexpectedElements: Long = 106238\nfpp: Double = 0.005\n"}]},"apps":[],"jobName":"paragraph_1574795615781_-883128518","id":"20191126-141335_189734483","dateCreated":"2019-11-26T14:13:35-0500","dateStarted":"2019-12-03T23:06:33-0500","dateFinished":"2019-12-03T23:06:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5251"},{"text":"// create the boom filter \"sbf\" on the model column of the df_B dataframe\nval sbf = df_B.stat.bloomFilter($\"model\", expectedElements,fpp)\n// broadcast the created bloom filter and let the broadcasted variable be \"a\"\nval a = spark.sparkContext.broadcast(sbf)","user":"anonymous","dateUpdated":"2019-12-03T23:06:33-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sbf: org.apache.spark.util.sketch.BloomFilter = org.apache.spark.util.sketch.BloomFilterImpl@965e6c5e\na: org.apache.spark.broadcast.Broadcast[org.apache.spark.util.sketch.BloomFilter] = Broadcast(17)\n"}]},"apps":[],"jobName":"paragraph_1574795622055_1396234269","id":"20191126-141342_876545542","dateCreated":"2019-11-26T14:13:42-0500","dateStarted":"2019-12-03T23:06:33-0500","dateFinished":"2019-12-03T23:06:34-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5252"},{"text":"// define a udf: \"might_contain\" which applies the boom filter which performs the filtering operation\ndef might_contain(f: org.apache.spark.util.sketch.BloomFilter) = udf((x: String) => \n  if(x != null) f.mightContain(x) else false)","user":"anonymous","dateUpdated":"2019-12-03T23:06:34-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"might_contain: (f: org.apache.spark.util.sketch.BloomFilter)org.apache.spark.sql.expressions.UserDefinedFunction\n"}]},"apps":[],"jobName":"paragraph_1574796632438_-1459707048","id":"20191126-143032_1876825251","dateCreated":"2019-11-26T14:30:32-0500","dateStarted":"2019-12-03T23:06:34-0500","dateFinished":"2019-12-03T23:06:35-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5253"},{"text":"// perform filtering in df_A based on the broadcasted bloom filter \"a\" and the created udf : \"might_contain\", this might give us certain false positives\nval df_result=df_A.where(might_contain(a.value)($\"model\"))\n// another way to perform filtering , either this approach or the one using udf can be used\nval x = df_A.rdd.filter(x => a.value.mightContain(x(2)))\n","user":"anonymous","dateUpdated":"2019-12-03T23:06:35-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_result: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, serial_number: string ... 127 more fields]\nx: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at filter at <console>:35\n"}]},"apps":[],"jobName":"paragraph_1574796713363_-1969501536","id":"20191126-143153_452900353","dateCreated":"2019-11-26T14:31:53-0500","dateStarted":"2019-12-03T23:06:35-0500","dateFinished":"2019-12-03T23:06:36-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5254"},{"text":"// df_result.show(5)\n// df_result.printSchema()\n// lets dipslay only 3 columns date, serial_number and model\nval columnNames = Seq(\"date\",\"serial_number\",\"model\")\nval result = df_result.select(columnNames.head, columnNames.tail: _*)\nresult.show()\n// df_result.select('date','serial_number','model')","user":"anonymous","dateUpdated":"2019-12-03T23:13:28-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+--------------+--------------------+\n|      date| serial_number|               model|\n+----------+--------------+--------------------+\n|2019-03-05|      Z305B2QN|         ST4000DM000|\n|2019-03-05|      ZJV0XJQ4|       ST12000NM0007|\n|2019-03-05|      ZJV0XJQ3|       ST12000NM0007|\n|2019-03-05|      ZJV0XJQ0|       ST12000NM0007|\n|2019-03-05|PL1331LAHG1S4H|HGST HMS5C4040ALE640|\n|2019-03-05|      ZA16NQJR|        ST8000NM0055|\n|2019-03-05|      ZJV02XWG|       ST12000NM0007|\n|2019-03-05|      ZJV1CSVX|       ST12000NM0007|\n|2019-03-05|      ZJV02XWA|       ST12000NM0007|\n|2019-03-05|      ZA18CEBS|        ST8000NM0055|\n|2019-03-05|      Z305DEMG|         ST4000DM000|\n|2019-03-05|      ZA130TTW|         ST8000DM002|\n|2019-03-05|      ZJV1CSVV|       ST12000NM0007|\n|2019-03-05|      ZA18CEBF|        ST8000NM0055|\n|2019-03-05|      ZJV02XWV|       ST12000NM0007|\n|2019-03-05|PL2331LAG9TEEJ|HGST HMS5C4040ALE640|\n|2019-03-05|PL2331LAH3WYAJ|HGST HMS5C4040BLE640|\n|2019-03-05|PL1331LAHG53YH|HGST HMS5C4040BLE640|\n|2019-03-05|  88Q0A0LGF97G| TOSHIBA MG07ACA14TA|\n|2019-03-05|PL2331LAHDUVVJ|HGST HMS5C4040BLE640|\n+----------+--------------+--------------------+\nonly showing top 20 rows\n\ncolumnNames: Seq[String] = List(date, serial_number, model)\nresult: org.apache.spark.sql.DataFrame = [date: string, serial_number: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1574796751452_-1155283954","id":"20191126-143231_1436669117","dateCreated":"2019-11-26T14:32:31-0500","dateStarted":"2019-12-03T23:13:28-0500","dateFinished":"2019-12-03T23:13:28-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5255"},{"text":"// print the numnber of columns after filtering using the udf function\ndf_result.count()","user":"anonymous","dateUpdated":"2019-12-03T23:06:36-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res8: Long = 9470149\n"}]},"apps":[],"jobName":"paragraph_1574796772204_-2133718699","id":"20191126-143252_1380162015","dateCreated":"2019-11-26T14:32:52-0500","dateStarted":"2019-12-03T23:06:37-0500","dateFinished":"2019-12-03T23:06:45-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5256"},{"text":"// val x_to_df=spark.createDataFrame(x)\n// print the number of columns after filtering using the rdd. filter approach, we notice that both the approaches yield the same number of rows\nx.count()","user":"anonymous","dateUpdated":"2019-12-03T23:06:45-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res9: Long = 9470149\n"}]},"apps":[],"jobName":"paragraph_1574796782662_38595194","id":"20191126-143302_1575654337","dateCreated":"2019-11-26T14:33:02-0500","dateStarted":"2019-12-03T23:06:45-0500","dateFinished":"2019-12-03T23:07:27-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5257"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1574797208520_-1021166710","id":"20191126-144008_1911215566","dateCreated":"2019-11-26T14:40:08-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5258","dateUpdated":"2019-12-03T23:07:27-0500"}],"name":"problem_4_midterm","id":"2EVRHAJV2","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}